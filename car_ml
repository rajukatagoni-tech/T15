# Very simple ML pipeline for car price prediction

import boto3
import pandas as pd
import numpy as np
import tempfile, joblib

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# -----------------------------
# Task 1: Load dataset from S3
# -----------------------------
bucket_name = "car-data123456"   # replace with your bucket name
folder = "car_cleaned_data"
file = "car_cleaned_data.csv"
s3_path = f"s3://{bucket_name}/{folder}/{file}"

df = pd.read_csv(s3_path)   # load into pandas
print(df.head())            # see first rows

# -----------------------------
# Task 2: Feature engineering
# -----------------------------
df["car_age"] = 2024 - df["year"]   # new column
df = df.drop(columns=["car name","year"])  # drop unwanted

# -----------------------------
# Task 3: Features and target
# -----------------------------
X = df.drop(columns=["selling_price"])   # inputs
y = df["selling_price"]                  # target

# -----------------------------
# Task 4: Preprocess data
# -----------------------------
# log transform to reduce skew
X["km_driven"] = np.log1p(X["km_driven"])
y_log = np.log1p(y)

# numeric + categorical columns
num_cols = X.select_dtypes(include=["int64","float64"]).columns
cat_cols = X.select_dtypes(include=["object"]).columns

preprocessor = ColumnTransformer([
    ("num", StandardScaler(), num_cols),
    ("cat", OneHotEncoder(handle_unknown="ignore"), cat_cols)
])

# -----------------------------
# Task 5 + 6: Pipeline + split
# -----------------------------
model = Pipeline([
    ("preprocessor", preprocessor),
    ("rf", RandomForestRegressor(random_state=8))
])

X_train, X_test, y_train, y_test = train_test_split(X, y_log, test_size=0.2, random_state=8)

# -----------------------------
# Task 7: Hyperparameter tuning
# -----------------------------
param_grid = {
    "rf__n_estimators": [100,200],
    "rf__max_depth": [None,10],
    "rf__min_samples_split": [2,5]
}
grid = GridSearchCV(model, param_grid, cv=3, scoring="r2")
grid.fit(X_train, y_train)

best_model = grid.best_estimator_
print("Best params:", grid.best_params_)

# -----------------------------
# Task 8 + 9: Train + predict
# -----------------------------
y_pred_log = best_model.predict(X_test)
y_pred = np.expm1(y_pred_log)   # reverse log
y_true = np.expm1(y_test)

# -----------------------------
# Task 10: Evaluate
# -----------------------------
print("MAE:", mean_absolute_error(y_true,y_pred))
print("MSE:", mean_squared_error(y_true,y_pred))
print("RMSE:", np.sqrt(mean_squared_error(y_true,y_pred)))
print("R2:", r2_score(y_true,y_pred))

# -----------------------------
# Task 11: Save model to S3
# -----------------------------
s3 = boto3.client("s3")
with tempfile.TemporaryFile() as tmp:
    joblib.dump(best_model, tmp)
    tmp.seek(0)
    s3.put_object(Bucket="car-data", Key="grid_search.pkl", Body=tmp.read())

print("âœ… Model saved to S3: car-data/grid_search.pkl")
